{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb4ee3ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install keras_tuner\n",
    "#!pip install --upgrade tensorflow-lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "242d2bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, brier_score_loss, log_loss, accuracy_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_lattice as tfl\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db85fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1. Data Loading and Preprocessing\n",
    "###############################################################################\n",
    "\n",
    "def load_model_data(dataset_path):\n",
    "    \"\"\"\n",
    "    Load CSV containing:\n",
    "      Season, LowerTeamID, HigherTeamID, Target\n",
    "      plus columns for your matchup features (e.g. ..._diff, ..._absdiff, etc.)\n",
    "    We'll drop [Season, LowerTeamID, HigherTeamID, Target, GameID, ID] from features.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    feature_cols = [\n",
    "        c for c in df.columns\n",
    "        if c not in ['Season','LowerTeamID','HigherTeamID','Target','GameID','ID']\n",
    "    ]\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df['Target'].copy()\n",
    "    return X, y, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b8aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 2. Metrics & Helpers\n",
    "###############################################################################\n",
    "\n",
    "def brier_score(y_true, y_prob):\n",
    "    \"\"\" Brier score = mean((y_true - y_prob)^2). Lower is better. \"\"\"\n",
    "    return brier_score_loss(y_true, y_prob)\n",
    "\n",
    "def log_loss_metric(y_true, y_prob):\n",
    "    \"\"\" scikit-learn log_loss. \"\"\"\n",
    "    return log_loss(y_true, y_prob)\n",
    "\n",
    "def accuracy_metric(y_true, y_pred_binary):\n",
    "    \"\"\" Standard accuracy comparing y_true vs binary predictions. \"\"\"\n",
    "    return accuracy_score(y_true, y_pred_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a5e4e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 3. LightGBM (Optimizing for Brier Score)\n",
    "###############################################################################\n",
    "\n",
    "def brier_scorer(estimator, X, y):\n",
    "    \"\"\"\n",
    "    Custom scikit-learn scorer for Brier:\n",
    "    we return -brier_score so GridSearchCV will 'maximize' it.\n",
    "    \"\"\"\n",
    "    prob = estimator.predict_proba(X)[:, 1]\n",
    "    return -brier_score_loss(y, prob)\n",
    "\n",
    "def train_lightgbm_model(X, y):\n",
    "    \"\"\"\n",
    "    Uses GridSearchCV to pick the best LightGBM hyperparams by Brier score.\n",
    "    Example param grid with monotonic constraints. \n",
    "    \"\"\"\n",
    "    # We'll do an 80/20 split for training vs. validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Suppose we assume features that end with \"_diff\" are monotonic +1\n",
    "    # If that doesn't match your data, adjust accordingly or set them all 0.\n",
    "    monotonic_constraints = []\n",
    "    for col in X_train.columns:\n",
    "        if (col == 'window_TO_avg_diff') or (col == 'window_PF_avg_diff'):\n",
    "            monotonic_constraints.append(-1)\n",
    "        elif col == 'window_clutch_count_diff':\n",
    "            monotonic_constraints.append(0)\n",
    "        elif col.endswith(\"_diff\"):\n",
    "            monotonic_constraints.append(1)\n",
    "        else:\n",
    "            monotonic_constraints.append(0)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [500, 1000],\n",
    "        'learning_rate': [0.01, 0.05],\n",
    "        'num_leaves': [31, 63],\n",
    "        'max_depth': [3, 5],\n",
    "        'min_child_samples': [10, 20],\n",
    "        'monotone_constraints': [monotonic_constraints]\n",
    "    }\n",
    "\n",
    "    scorer = make_scorer(brier_scorer, greater_is_better=True)\n",
    "    lgb_model = lgb.LGBMClassifier(objective='binary', random_state=42)\n",
    "    grid = GridSearchCV(\n",
    "        estimator=lgb_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scorer,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train,\n",
    "             eval_set=[(X_val, y_val)],\n",
    "             early_stopping_rounds=50,\n",
    "             verbose=False)\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    # Evaluate on validation\n",
    "    val_probs = best_model.predict_proba(X_val)[:,1]\n",
    "    val_preds = (val_probs > 0.5).astype(int)\n",
    "    val_brier = brier_score(y_val, val_probs)\n",
    "    val_logloss = log_loss_metric(y_val, val_probs)\n",
    "    val_acc = accuracy_metric(y_val, val_preds)\n",
    "\n",
    "    print(\"Best LightGBM hyperparams:\", grid.best_params_)\n",
    "    print(f\"LightGBM val Brier: {val_brier:.4f}, LogLoss: {val_logloss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8866a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 4. Logistic Regression (scikit-learn) - Brier Score\n",
    "###############################################################################\n",
    "\n",
    "def train_logistic_regression(X, y):\n",
    "    \"\"\"\n",
    "    scikit-learn LogisticRegression, small param grid, picking best by Brier score.\n",
    "    \"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'solver': ['lbfgs', 'liblinear'],\n",
    "        'max_iter': [100, 300]\n",
    "    }\n",
    "    scorer = make_scorer(brier_scorer, greater_is_better=True)\n",
    "    model = LogisticRegression(random_state=42)\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scorer,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "    val_probs = best_model.predict_proba(X_val)[:, 1]\n",
    "    val_preds = (val_probs > 0.5).astype(int)\n",
    "    val_brier = brier_score(y_val, val_probs)\n",
    "    val_ll = log_loss_metric(y_val, val_probs)\n",
    "    val_acc = accuracy_metric(y_val, val_preds)\n",
    "\n",
    "    print(\"Best LogisticRegression hyperparams:\", grid.best_params_)\n",
    "    print(f\"LogReg val Brier: {val_brier:.4f}, LogLoss: {val_ll:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "900091b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 5. TensorFlow Lattice (Older PWLCalibration API, custom monotonicities)\n",
    "###############################################################################\n",
    "\n",
    "# We'll define a custom Brier metric in TF\n",
    "def brier_score_tf(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "def build_tf_lattice_custom_monotonic_model_legacy_lattice_sizes(hp, feature_names, X_train):\n",
    "    \"\"\"\n",
    "    Older TF Lattice code requiring 'lattice_sizes' in Lattice(...).\n",
    "    We'll do:\n",
    "      - For each feature, a PWLCalibration with 'input_keypoints' array (no num_keypoints param).\n",
    "      - Then a Lattice layer with lattice_sizes=[2,2,...] (or [3,3,...]) plus monotonicities.\n",
    "    \"\"\"\n",
    "\n",
    "    # Suppose we define some custom monotonic map:\n",
    "    custom_monotonic_map = {\n",
    "        'window_TO_avg_diff': 'decreasing',\n",
    "        'window_PF_avg_diff': 'decreasing',\n",
    "        'window_clutch_count_diff': 'none'\n",
    "    }\n",
    "\n",
    "    # We'll pretend we have a tuner param for how many keypoints to use in PWLCalibration\n",
    "    num_keypoints = hp.Int('num_keypoints', min_value=5, max_value=15, step=5, default=10)\n",
    "    lr = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log', default=1e-3)\n",
    "\n",
    "    inputs = {}\n",
    "    calibrators = []\n",
    "    for feat in feature_names:\n",
    "        inputs[feat] = tf.keras.Input(shape=(1,), name=feat)\n",
    "\n",
    "        # monotonic direction for PWL\n",
    "        if feat in custom_monotonic_map:\n",
    "            this_monotonic = custom_monotonic_map[feat]\n",
    "        else:\n",
    "            this_monotonic = 'increasing'\n",
    "\n",
    "        # build input_keypoints array\n",
    "        f_min = float(X_train[feat].min())\n",
    "        f_max = float(X_train[feat].max())\n",
    "        keypoints = np.linspace(f_min, f_max, num_keypoints)\n",
    "\n",
    "        # PWLCalibration older signature\n",
    "        c = tfl.layers.PWLCalibration(\n",
    "            input_keypoints=keypoints,\n",
    "            units=1,\n",
    "            output_min=0.0,\n",
    "            output_max=1.0,\n",
    "            clamp_min=False,\n",
    "            clamp_max=False,\n",
    "            monotonicity=this_monotonic\n",
    "        )(inputs[feat])\n",
    "        calibrators.append(c)\n",
    "\n",
    "    # Concatenate calibrator outputs\n",
    "    concat_calibrators = tf.keras.layers.Concatenate()(calibrators)\n",
    "\n",
    "    # Next, older Lattice requires 'lattice_sizes'\n",
    "    # e.g. if you have len(feature_names)=10, you might do [2]*10 => each dimension has 2 vertices\n",
    "    n_dims = len(feature_names)\n",
    "    \n",
    "    # Build the integer monotonicities for Lattice: +1 => increasing, -1 => decreasing, 0 => none\n",
    "    lattice_monotonicities = []\n",
    "    for feat in feature_names:\n",
    "        if feat in custom_monotonic_map:\n",
    "            if custom_monotonic_map[feat] == 'increasing':\n",
    "                lattice_monotonicities.append(1)\n",
    "            elif custom_monotonic_map[feat] == 'decreasing':\n",
    "                lattice_monotonicities.append(0)\n",
    "            else:\n",
    "                lattice_monotonicities.append(0)\n",
    "        else:\n",
    "            lattice_monotonicities.append(1)\n",
    "\n",
    "    # Suppose we want 2 vertices per dimension (2^n total corners).\n",
    "    # If you have fewer features or need more resolution, you can try [3]*n_dims.\n",
    "    lattice_out = tfl.layers.Lattice(\n",
    "        lattice_sizes=[2]*n_dims,\n",
    "        monotonicities=lattice_monotonicities,\n",
    "        units=1  # for binary classification\n",
    "    )(concat_calibrators)\n",
    "\n",
    "    # Final output => Sigmoid for probability\n",
    "    outputs = tf.keras.layers.Activation('sigmoid')(lattice_out)\n",
    "    model = tf.keras.Model(inputs=list(inputs.values()), outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_tf_lattice_model_legacy_sizes(X, y):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    feature_names = X.columns.tolist()\n",
    "\n",
    "    # Keras Tuner \n",
    "    import keras_tuner as kt\n",
    "    \n",
    "    # We'll define a custom Brier metric in TF:\n",
    "    def brier_score_tf(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "    train_dict = {col: X_train[col].values for col in feature_names}\n",
    "    val_dict = {col: X_val[col].values for col in feature_names}\n",
    "\n",
    "    def model_builder(hp):\n",
    "        model = build_tf_lattice_custom_monotonic_model_legacy_lattice_sizes(\n",
    "            hp, feature_names, X_train\n",
    "        )\n",
    "        # recompile with brier metric\n",
    "        model.compile(\n",
    "            optimizer=model.optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=[\n",
    "                'accuracy',\n",
    "                tf.keras.metrics.BinaryCrossentropy(name='log_loss'),\n",
    "                brier_score_tf\n",
    "            ]\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    tuner = kt.RandomSearch(\n",
    "        model_builder,\n",
    "        objective=kt.Objective('val_brier_score_tf', direction='min'),\n",
    "        max_trials=5,\n",
    "        executions_per_trial=1,\n",
    "        project_name='tf_lattice_sizes',\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_brier_score_tf', patience=5, mode='min')\n",
    "\n",
    "    tuner.search(\n",
    "        train_dict, y_train,\n",
    "        validation_data=(val_dict, y_val),\n",
    "        epochs=50,\n",
    "        batch_size=128,\n",
    "        callbacks=[stop_early],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    best_model = tuner.hypermodel.build(best_hps)\n",
    "    best_model.fit(\n",
    "        train_dict, y_train,\n",
    "        validation_data=(val_dict, y_val),\n",
    "        epochs=50,\n",
    "        batch_size=128,\n",
    "        callbacks=[stop_early],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Evaluate final\n",
    "    res = best_model.evaluate(val_dict, y_val, verbose=0)\n",
    "    names = best_model.metrics_names\n",
    "    res_dict = dict(zip(names, res))\n",
    "    print(\"Best TF Lattice hyperparams:\", best_hps.values)\n",
    "    print(f\"Brier: {res_dict['brier_score_tf']:.4f}, log_loss: {res_dict['log_loss']:.4f}, acc: {res_dict['accuracy']:.4f}\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a233354",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 6. Ensemble\n",
    "###############################################################################\n",
    "\n",
    "def ensemble_predict_proba(X, model_lgb, model_tf, model_pt, pt_device):\n",
    "    # LightGBM\n",
    "    preds_lgb = model_lgb.predict_proba(X)[:,1]\n",
    "    # TF Lattice\n",
    "    tf_inputs = {col: X[col].values for col in X.columns}\n",
    "    preds_tf = model_tf.predict(tf_inputs).flatten()\n",
    "    # PyTorch\n",
    "    preds_pt = predict_with_pytorch_model(model_pt, pt_device, X)\n",
    "\n",
    "    # Simple average\n",
    "    ensemble_probs = (preds_lgb + preds_tf + preds_pt) / 3.0\n",
    "    return ensemble_probs\n",
    "\n",
    "def predict_submission_ensemble(dataset_path, model_lgb, model_tf, model_pt, pt_device, out_filename='submission_ensemble.csv'):\n",
    "    \"\"\"\n",
    "    Create a submission for the ensemble by averaging predicted probabilities.\n",
    "    The test dataset must have [Season, LowerTeamID, HigherTeamID], plus feature columns.\n",
    "    We'll produce ID = \"SSSS_XXXX_YYYY\" and Pred = ensemble probability.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    df['ID'] = df.apply(lambda row: f\"{int(row['Season']):04d}_{int(row['LowerTeamID']):04d}_{int(row['HigherTeamID']):04d}\", axis=1)\n",
    "\n",
    "    feature_cols = [\n",
    "        c for c in df.columns\n",
    "        if c not in ['Season','LowerTeamID','HigherTeamID','Target','GameID','ID']\n",
    "    ]\n",
    "    X_test = df[feature_cols].copy()\n",
    "\n",
    "    preds = ensemble_predict_proba(X_test, model_lgb, model_tf, model_pt, pt_device)\n",
    "    submission = pd.DataFrame({'ID': df['ID'], 'Pred': preds})\n",
    "    submission.to_csv(out_filename, index=False)\n",
    "    print(f\"Ensemble submission saved to {out_filename}\")\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e945e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 7. Predict Submission for Single Model\n",
    "###############################################################################\n",
    "\n",
    "def predict_submission(model, dataset_path, model_type='lgb', pt_device=None, out_filename='submission.csv'):\n",
    "    \"\"\"\n",
    "    Single-model submission. 'lgb' => LightGBM, 'tf' => TF Lattice, 'pt' => PyTorch LR.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    df['ID'] = df.apply(lambda row: f\"{int(row['Season']):04d}_{int(row['LowerTeamID']):04d}_{int(row['HigherTeamID']):04d}\", axis=1)\n",
    "\n",
    "    feature_cols = [\n",
    "        c for c in df.columns\n",
    "        if c not in ['Season','LowerTeamID','HigherTeamID','Target','GameID','ID']\n",
    "    ]\n",
    "    X_test = df[feature_cols].copy()\n",
    "\n",
    "    if model_type == 'lgb':\n",
    "        probs = model.predict_proba(X_test)[:,1]\n",
    "    elif model_type == 'tf':\n",
    "        test_inputs = {col: X_test[col].values for col in X_test.columns}\n",
    "        probs = model.predict(test_inputs).flatten()\n",
    "    elif model_type == 'pt':\n",
    "        probs = predict_with_pytorch_model(model, pt_device, X_test)\n",
    "    else:\n",
    "        raise ValueError(\"model_type must be 'lgb', 'tf', or 'pt'\")\n",
    "\n",
    "    submission = pd.DataFrame({'ID': df['ID'], 'Pred': probs})\n",
    "    submission.to_csv(out_filename, index=False)\n",
    "    print(f\"{model_type} submission saved to {out_filename}\")\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75c06571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dataset path if needed:\n",
    "dataset_path = \"7_game_window_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c272624",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, df = load_model_data(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0809e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window_score_avg_diff</th>\n",
       "      <th>window_FG_pct_diff</th>\n",
       "      <th>window_3P_pct_diff</th>\n",
       "      <th>window_FT_pct_diff</th>\n",
       "      <th>window_off_eff_diff</th>\n",
       "      <th>window_Ast_avg_diff</th>\n",
       "      <th>window_TO_avg_diff</th>\n",
       "      <th>window_Stl_avg_diff</th>\n",
       "      <th>window_Blk_avg_diff</th>\n",
       "      <th>window_PF_avg_diff</th>\n",
       "      <th>window_OR_avg_diff</th>\n",
       "      <th>window_DR_avg_diff</th>\n",
       "      <th>window_clutch_count_diff</th>\n",
       "      <th>window_clutch_win_pct_diff</th>\n",
       "      <th>window_clutch_margin_avg_diff</th>\n",
       "      <th>window_clutch_score_avg_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-13.285714</td>\n",
       "      <td>0.019152</td>\n",
       "      <td>-0.080009</td>\n",
       "      <td>-0.094506</td>\n",
       "      <td>-0.097819</td>\n",
       "      <td>-1.857143</td>\n",
       "      <td>-2.142857</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>-10.285714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-24.000000</td>\n",
       "      <td>-0.025815</td>\n",
       "      <td>-0.159120</td>\n",
       "      <td>-0.167807</td>\n",
       "      <td>-0.192594</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-4.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-4.714286</td>\n",
       "      <td>-7.285714</td>\n",
       "      <td>-5.428571</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-17.428571</td>\n",
       "      <td>-0.086854</td>\n",
       "      <td>-0.049437</td>\n",
       "      <td>0.014608</td>\n",
       "      <td>-0.101582</td>\n",
       "      <td>-3.142857</td>\n",
       "      <td>-6.714286</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>-5.285714</td>\n",
       "      <td>-1.428571</td>\n",
       "      <td>-6.142857</td>\n",
       "      <td>-6.857143</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-10.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-19.428571</td>\n",
       "      <td>-0.051309</td>\n",
       "      <td>0.101969</td>\n",
       "      <td>-0.085944</td>\n",
       "      <td>-0.064134</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-4.857143</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>-6.571429</td>\n",
       "      <td>-5.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-13.285714</td>\n",
       "      <td>0.061963</td>\n",
       "      <td>0.110200</td>\n",
       "      <td>-0.044643</td>\n",
       "      <td>0.128834</td>\n",
       "      <td>3.142857</td>\n",
       "      <td>-7.142857</td>\n",
       "      <td>-1.714286</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-10.714286</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89133</th>\n",
       "      <td>-3.142857</td>\n",
       "      <td>-0.008950</td>\n",
       "      <td>0.009480</td>\n",
       "      <td>-0.020163</td>\n",
       "      <td>0.033406</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>-4.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-4.285714</td>\n",
       "      <td>-0.571429</td>\n",
       "      <td>-1.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89134</th>\n",
       "      <td>-4.714286</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.010297</td>\n",
       "      <td>-0.075225</td>\n",
       "      <td>-0.011430</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>-1.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>-3.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89135</th>\n",
       "      <td>13.428571</td>\n",
       "      <td>0.061411</td>\n",
       "      <td>0.059355</td>\n",
       "      <td>-0.020468</td>\n",
       "      <td>0.157346</td>\n",
       "      <td>5.571429</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89136</th>\n",
       "      <td>14.285714</td>\n",
       "      <td>0.032109</td>\n",
       "      <td>-0.052083</td>\n",
       "      <td>-0.014807</td>\n",
       "      <td>0.041815</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.285714</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>5.428571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>22.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89137</th>\n",
       "      <td>1.714286</td>\n",
       "      <td>0.040633</td>\n",
       "      <td>-0.027283</td>\n",
       "      <td>-0.196894</td>\n",
       "      <td>0.057372</td>\n",
       "      <td>-1.285714</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>-1.285714</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>2.714286</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>1.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89138 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       window_score_avg_diff  window_FG_pct_diff  window_3P_pct_diff  \\\n",
       "0                 -13.285714            0.019152           -0.080009   \n",
       "1                 -24.000000           -0.025815           -0.159120   \n",
       "2                 -17.428571           -0.086854           -0.049437   \n",
       "3                 -19.428571           -0.051309            0.101969   \n",
       "4                 -13.285714            0.061963            0.110200   \n",
       "...                      ...                 ...                 ...   \n",
       "89133              -3.142857           -0.008950            0.009480   \n",
       "89134              -4.714286            0.001387            0.010297   \n",
       "89135              13.428571            0.061411            0.059355   \n",
       "89136              14.285714            0.032109           -0.052083   \n",
       "89137               1.714286            0.040633           -0.027283   \n",
       "\n",
       "       window_FT_pct_diff  window_off_eff_diff  window_Ast_avg_diff  \\\n",
       "0               -0.094506            -0.097819            -1.857143   \n",
       "1               -0.167807            -0.192594            -0.714286   \n",
       "2                0.014608            -0.101582            -3.142857   \n",
       "3               -0.085944            -0.064134            -2.000000   \n",
       "4               -0.044643             0.128834             3.142857   \n",
       "...                   ...                  ...                  ...   \n",
       "89133           -0.020163             0.033406             1.285714   \n",
       "89134           -0.075225            -0.011430            -2.000000   \n",
       "89135           -0.020468             0.157346             5.571429   \n",
       "89136           -0.014807             0.041815             2.571429   \n",
       "89137           -0.196894             0.057372            -1.285714   \n",
       "\n",
       "       window_TO_avg_diff  window_Stl_avg_diff  window_Blk_avg_diff  \\\n",
       "0               -2.142857            -0.857143            -0.142857   \n",
       "1               -4.285714             0.000000            -0.428571   \n",
       "2               -6.714286             1.285714            -5.285714   \n",
       "3               -4.857143             0.142857            -2.000000   \n",
       "4               -7.142857            -1.714286            -2.000000   \n",
       "...                   ...                  ...                  ...   \n",
       "89133           -4.571429             0.000000             0.142857   \n",
       "89134            1.714286            -1.285714             1.000000   \n",
       "89135           -0.142857             0.428571             1.857143   \n",
       "89136            3.857143             0.285714             1.000000   \n",
       "89137           -0.142857            -1.285714             0.571429   \n",
       "\n",
       "       window_PF_avg_diff  window_OR_avg_diff  window_DR_avg_diff  \\\n",
       "0                0.857143           -9.000000          -10.285714   \n",
       "1               -4.714286           -7.285714           -5.428571   \n",
       "2               -1.428571           -6.142857           -6.857143   \n",
       "3                0.428571           -6.571429           -5.285714   \n",
       "4               -0.428571          -10.714286           -8.000000   \n",
       "...                   ...                 ...                 ...   \n",
       "89133           -4.285714           -0.571429           -1.714286   \n",
       "89134           -0.857143            0.857143            3.571429   \n",
       "89135           -0.714286            1.142857            1.142857   \n",
       "89136            5.285714            2.428571            5.428571   \n",
       "89137            2.714286            2.428571            1.285714   \n",
       "\n",
       "       window_clutch_count_diff  window_clutch_win_pct_diff  \\\n",
       "0                           1.0                   -0.500000   \n",
       "1                           2.0                    0.333333   \n",
       "2                          -1.0                   -0.166667   \n",
       "3                           0.0                    0.000000   \n",
       "4                           0.0                    0.000000   \n",
       "...                         ...                         ...   \n",
       "89133                       0.0                    0.500000   \n",
       "89134                       0.0                   -0.500000   \n",
       "89135                       1.0                    0.666667   \n",
       "89136                       1.0                    0.333333   \n",
       "89137                      -2.0                   -0.500000   \n",
       "\n",
       "       window_clutch_margin_avg_diff  window_clutch_score_avg_diff  \n",
       "0                          -1.500000                      9.000000  \n",
       "1                           4.000000                    -19.000000  \n",
       "2                           0.500000                    -10.500000  \n",
       "3                           0.000000                      0.000000  \n",
       "4                           0.000000                      0.000000  \n",
       "...                              ...                           ...  \n",
       "89133                       5.000000                    -16.000000  \n",
       "89134                      -3.750000                     -3.750000  \n",
       "89135                       4.000000                      2.166667  \n",
       "89136                       1.666667                     22.166667  \n",
       "89137                      -6.000000                      1.750000  \n",
       "\n",
       "[89138 rows x 16 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e136ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Train LightGBM\n",
    "############################################################################\n",
    "print(\"\\n==== Training LightGBM (Optimizing Brier) ====\")\n",
    "lgb_model = train_lightgbm_model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6958210d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 26m 11s]\n",
      "val_brier_score_tf: 0.22234989702701569\n",
      "\n",
      "Best val_brier_score_tf So Far: 0.22234989702701569\n",
      "Total elapsed time: 00h 26m 11s\n",
      "\n",
      "Search: Running Trial #2\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "15                |5                 |num_keypoints\n",
      "0.00079908        |0.00011655        |learning_rate\n",
      "\n",
      "Epoch 1/50\n",
      "558/558 [==============================] - 35s 56ms/step - loss: 0.6708 - accuracy: 0.6049 - log_loss: 0.6708 - brier_score_tf: 0.2389 - val_loss: 0.6560 - val_accuracy: 0.6306 - val_log_loss: 0.6560 - val_brier_score_tf: 0.2317\n",
      "Epoch 2/50\n",
      "558/558 [==============================] - 30s 54ms/step - loss: 0.6508 - accuracy: 0.6322 - log_loss: 0.6508 - brier_score_tf: 0.2292 - val_loss: 0.6489 - val_accuracy: 0.6356 - val_log_loss: 0.6489 - val_brier_score_tf: 0.2284\n",
      "Epoch 3/50\n",
      "558/558 [==============================] - 30s 54ms/step - loss: 0.6461 - accuracy: 0.6350 - log_loss: 0.6461 - brier_score_tf: 0.2270 - val_loss: 0.6458 - val_accuracy: 0.6359 - val_log_loss: 0.6458 - val_brier_score_tf: 0.2269\n",
      "Epoch 4/50\n",
      "558/558 [==============================] - 33s 59ms/step - loss: 0.6432 - accuracy: 0.6368 - log_loss: 0.6432 - brier_score_tf: 0.2257 - val_loss: 0.6432 - val_accuracy: 0.6387 - val_log_loss: 0.6432 - val_brier_score_tf: 0.2257\n",
      "Epoch 5/50\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.6409 - accuracy: 0.6395 - log_loss: 0.6409 - brier_score_tf: 0.2247 - val_loss: 0.6415 - val_accuracy: 0.6396 - val_log_loss: 0.6415 - val_brier_score_tf: 0.2250\n",
      "Epoch 6/50\n",
      "558/558 [==============================] - 33s 59ms/step - loss: 0.6396 - accuracy: 0.6406 - log_loss: 0.6396 - brier_score_tf: 0.2241 - val_loss: 0.6402 - val_accuracy: 0.6419 - val_log_loss: 0.6402 - val_brier_score_tf: 0.2244\n",
      "Epoch 7/50\n",
      "558/558 [==============================] - 32s 58ms/step - loss: 0.6380 - accuracy: 0.6409 - log_loss: 0.6380 - brier_score_tf: 0.2234 - val_loss: 0.6390 - val_accuracy: 0.6426 - val_log_loss: 0.6390 - val_brier_score_tf: 0.2238\n",
      "Epoch 8/50\n",
      "558/558 [==============================] - 32s 57ms/step - loss: 0.6369 - accuracy: 0.6422 - log_loss: 0.6369 - brier_score_tf: 0.2230 - val_loss: 0.6377 - val_accuracy: 0.6433 - val_log_loss: 0.6377 - val_brier_score_tf: 0.2232\n",
      "Epoch 9/50\n",
      "558/558 [==============================] - 32s 57ms/step - loss: 0.6357 - accuracy: 0.6431 - log_loss: 0.6357 - brier_score_tf: 0.2223 - val_loss: 0.6366 - val_accuracy: 0.6437 - val_log_loss: 0.6366 - val_brier_score_tf: 0.2227\n",
      "Epoch 10/50\n",
      "558/558 [==============================] - 32s 58ms/step - loss: 0.6345 - accuracy: 0.6429 - log_loss: 0.6345 - brier_score_tf: 0.2219 - val_loss: 0.6353 - val_accuracy: 0.6453 - val_log_loss: 0.6353 - val_brier_score_tf: 0.2222\n",
      "Epoch 11/50\n",
      "558/558 [==============================] - 34s 60ms/step - loss: 0.6335 - accuracy: 0.6430 - log_loss: 0.6335 - brier_score_tf: 0.2214 - val_loss: 0.6345 - val_accuracy: 0.6452 - val_log_loss: 0.6345 - val_brier_score_tf: 0.2218\n",
      "Epoch 12/50\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.6327 - accuracy: 0.6437 - log_loss: 0.6327 - brier_score_tf: 0.2211 - val_loss: 0.6335 - val_accuracy: 0.6467 - val_log_loss: 0.6335 - val_brier_score_tf: 0.2213\n",
      "Epoch 13/50\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 0.6319 - accuracy: 0.6444 - log_loss: 0.6319 - brier_score_tf: 0.2208 - val_loss: 0.6332 - val_accuracy: 0.6485 - val_log_loss: 0.6332 - val_brier_score_tf: 0.2212\n",
      "Epoch 14/50\n",
      "558/558 [==============================] - 32s 58ms/step - loss: 0.6314 - accuracy: 0.6434 - log_loss: 0.6314 - brier_score_tf: 0.2205 - val_loss: 0.6326 - val_accuracy: 0.6449 - val_log_loss: 0.6326 - val_brier_score_tf: 0.2210\n",
      "Epoch 15/50\n",
      "558/558 [==============================] - 31s 56ms/step - loss: 0.6307 - accuracy: 0.6442 - log_loss: 0.6307 - brier_score_tf: 0.2203 - val_loss: 0.6319 - val_accuracy: 0.6463 - val_log_loss: 0.6319 - val_brier_score_tf: 0.2206\n",
      "Epoch 16/50\n",
      "558/558 [==============================] - 32s 58ms/step - loss: 0.6299 - accuracy: 0.6435 - log_loss: 0.6299 - brier_score_tf: 0.2199 - val_loss: 0.6315 - val_accuracy: 0.6456 - val_log_loss: 0.6315 - val_brier_score_tf: 0.2205\n",
      "Epoch 17/50\n",
      "558/558 [==============================] - 32s 58ms/step - loss: 0.6297 - accuracy: 0.6442 - log_loss: 0.6297 - brier_score_tf: 0.2199 - val_loss: 0.6307 - val_accuracy: 0.6480 - val_log_loss: 0.6307 - val_brier_score_tf: 0.2201\n",
      "Epoch 18/50\n",
      "558/558 [==============================] - 34s 60ms/step - loss: 0.6289 - accuracy: 0.6449 - log_loss: 0.6289 - brier_score_tf: 0.2195 - val_loss: 0.6307 - val_accuracy: 0.6488 - val_log_loss: 0.6307 - val_brier_score_tf: 0.2201\n",
      "Epoch 19/50\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.6285 - accuracy: 0.6445 - log_loss: 0.6285 - brier_score_tf: 0.2194 - val_loss: 0.6299 - val_accuracy: 0.6488 - val_log_loss: 0.6299 - val_brier_score_tf: 0.2198\n",
      "Epoch 20/50\n",
      "558/558 [==============================] - 32s 57ms/step - loss: 0.6281 - accuracy: 0.6447 - log_loss: 0.6281 - brier_score_tf: 0.2191 - val_loss: 0.6298 - val_accuracy: 0.6476 - val_log_loss: 0.6298 - val_brier_score_tf: 0.2197\n",
      "Epoch 21/50\n",
      "558/558 [==============================] - 34s 60ms/step - loss: 0.6278 - accuracy: 0.6445 - log_loss: 0.6278 - brier_score_tf: 0.2190 - val_loss: 0.6292 - val_accuracy: 0.6489 - val_log_loss: 0.6292 - val_brier_score_tf: 0.2195\n",
      "Epoch 22/50\n",
      "558/558 [==============================] - 33s 59ms/step - loss: 0.6274 - accuracy: 0.6447 - log_loss: 0.6274 - brier_score_tf: 0.2189 - val_loss: 0.6290 - val_accuracy: 0.6483 - val_log_loss: 0.6290 - val_brier_score_tf: 0.2194\n",
      "Epoch 23/50\n",
      "558/558 [==============================] - 36s 64ms/step - loss: 0.6270 - accuracy: 0.6450 - log_loss: 0.6270 - brier_score_tf: 0.2187 - val_loss: 0.6285 - val_accuracy: 0.6491 - val_log_loss: 0.6285 - val_brier_score_tf: 0.2192\n",
      "Epoch 24/50\n",
      "558/558 [==============================] - 34s 62ms/step - loss: 0.6268 - accuracy: 0.6454 - log_loss: 0.6268 - brier_score_tf: 0.2186 - val_loss: 0.6290 - val_accuracy: 0.6474 - val_log_loss: 0.6290 - val_brier_score_tf: 0.2194\n",
      "Epoch 25/50\n",
      "558/558 [==============================] - 31s 55ms/step - loss: 0.6265 - accuracy: 0.6454 - log_loss: 0.6265 - brier_score_tf: 0.2186 - val_loss: 0.6283 - val_accuracy: 0.6490 - val_log_loss: 0.6283 - val_brier_score_tf: 0.2191\n",
      "Epoch 26/50\n",
      "558/558 [==============================] - 30s 54ms/step - loss: 0.6262 - accuracy: 0.6457 - log_loss: 0.6262 - brier_score_tf: 0.2184 - val_loss: 0.6282 - val_accuracy: 0.6497 - val_log_loss: 0.6282 - val_brier_score_tf: 0.2191\n",
      "Epoch 27/50\n",
      "558/558 [==============================] - 30s 53ms/step - loss: 0.6259 - accuracy: 0.6459 - log_loss: 0.6259 - brier_score_tf: 0.2182 - val_loss: 0.6280 - val_accuracy: 0.6472 - val_log_loss: 0.6280 - val_brier_score_tf: 0.2190\n",
      "Epoch 28/50\n",
      "558/558 [==============================] - 30s 53ms/step - loss: 0.6257 - accuracy: 0.6460 - log_loss: 0.6257 - brier_score_tf: 0.2182 - val_loss: 0.6283 - val_accuracy: 0.6466 - val_log_loss: 0.6283 - val_brier_score_tf: 0.2191\n",
      "Epoch 29/50\n",
      "558/558 [==============================] - 30s 54ms/step - loss: 0.6256 - accuracy: 0.6461 - log_loss: 0.6256 - brier_score_tf: 0.2182 - val_loss: 0.6279 - val_accuracy: 0.6489 - val_log_loss: 0.6279 - val_brier_score_tf: 0.2190\n",
      "Epoch 30/50\n",
      "558/558 [==============================] - 30s 54ms/step - loss: 0.6253 - accuracy: 0.6461 - log_loss: 0.6253 - brier_score_tf: 0.2180 - val_loss: 0.6272 - val_accuracy: 0.6487 - val_log_loss: 0.6272 - val_brier_score_tf: 0.2187\n",
      "Epoch 31/50\n",
      "558/558 [==============================] - 30s 54ms/step - loss: 0.6252 - accuracy: 0.6462 - log_loss: 0.6252 - brier_score_tf: 0.2179 - val_loss: 0.6270 - val_accuracy: 0.6489 - val_log_loss: 0.6270 - val_brier_score_tf: 0.2186\n",
      "Epoch 32/50\n",
      "558/558 [==============================] - 30s 55ms/step - loss: 0.6251 - accuracy: 0.6463 - log_loss: 0.6251 - brier_score_tf: 0.2180 - val_loss: 0.6269 - val_accuracy: 0.6489 - val_log_loss: 0.6269 - val_brier_score_tf: 0.2186\n",
      "Epoch 33/50\n",
      "558/558 [==============================] - 31s 55ms/step - loss: 0.6248 - accuracy: 0.6460 - log_loss: 0.6248 - brier_score_tf: 0.2178 - val_loss: 0.6266 - val_accuracy: 0.6493 - val_log_loss: 0.6266 - val_brier_score_tf: 0.2184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50\n",
      "558/558 [==============================] - 31s 55ms/step - loss: 0.6245 - accuracy: 0.6465 - log_loss: 0.6245 - brier_score_tf: 0.2177 - val_loss: 0.6269 - val_accuracy: 0.6465 - val_log_loss: 0.6269 - val_brier_score_tf: 0.2186\n",
      "Epoch 35/50\n",
      "558/558 [==============================] - 30s 54ms/step - loss: 0.6244 - accuracy: 0.6462 - log_loss: 0.6244 - brier_score_tf: 0.2176 - val_loss: 0.6265 - val_accuracy: 0.6487 - val_log_loss: 0.6265 - val_brier_score_tf: 0.2184\n",
      "Epoch 36/50\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.6243 - accuracy: 0.6463 - log_loss: 0.6243 - brier_score_tf: 0.2176"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# Train TF Lattice\n",
    "############################################################################\n",
    "print(\"\\n=== Training TF Lattice (Older PWLCalibration) ===\")\n",
    "tf_lattice_model = train_tf_lattice_model_legacy_sizes(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c970eb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Training PyTorch Logistic Regression (Optimizing Brier) ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sid22\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:106: UserWarning: \n",
      "NVIDIA GeForce RTX 3080 Ti with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 3080 Ti GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# Train PyTorch Logistic Regression\n",
    "############################################################################\n",
    "print(\"\\n==== Training PyTorch Logistic Regression (Optimizing Brier) ====\")\n",
    "pt_model, pt_device = train_pytorch_logreg(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a521d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Generate Submission\n",
    "############################################################################\n",
    "# Typically you'd have a separate \"test\" or future dataset for 2025 predictions\n",
    "# but here we'll just reuse the same dataset for demonstration.\n",
    "print(\"\\n==== Generating Submissions ====\")\n",
    "predict_submission(lgb_model, dataset_path, model_type='lgb', out_filename='submission_lgb.csv')\n",
    "predict_submission(tf_lattice_model, dataset_path, model_type='tf', out_filename='submission_tf.csv')\n",
    "predict_submission(pt_model, dataset_path, model_type='pt', pt_device=pt_device, out_filename='submission_pt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783467a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057fc834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b31df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e5620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
