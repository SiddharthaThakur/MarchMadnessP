{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb4ee3ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.12\n"
     ]
    }
   ],
   "source": [
    "#!pip install keras_tuner\n",
    "#!pip install --upgrade tensorflow-lattice\n",
    "#!pip install tensorflow==2.8.0\n",
    "#!pip install lightgbm\n",
    "#!pip install xgboost\n",
    "#!pip show tensorflow\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "242d2bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, brier_score_loss, log_loss, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_lattice as tfl\n",
    "import keras_tuner as kt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14a7868b-568d-4208-a2fb-502afb69c37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db85fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1. Data Loading and Preprocessing\n",
    "###############################################################################\n",
    "\n",
    "def load_model_data(dataset_path):\n",
    "    \"\"\"\n",
    "    Load CSV containing:\n",
    "      Season, LowerTeamID, HigherTeamID, Target\n",
    "      plus columns for your matchup features (e.g. ..._diff, ..._absdiff, etc.)\n",
    "    We'll drop [Season, LowerTeamID, HigherTeamID, Target, GameID, ID] from features.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    feature_cols = [\n",
    "        c for c in df.columns\n",
    "        if c not in ['Season','LowerTeamID','HigherTeamID','Target','GameID','ID']\n",
    "    ]\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df['Target'].copy()\n",
    "    return X, y, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b8aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 2. Metrics & Helpers\n",
    "###############################################################################\n",
    "\n",
    "def brier_score(y_true, y_prob):\n",
    "    \"\"\" Brier score = mean((y_true - y_prob)^2). Lower is better. \"\"\"\n",
    "    return brier_score_loss(y_true, y_prob)\n",
    "\n",
    "def log_loss_metric(y_true, y_prob):\n",
    "    \"\"\" scikit-learn log_loss. \"\"\"\n",
    "    return log_loss(y_true, y_prob)\n",
    "\n",
    "def accuracy_metric(y_true, y_pred_binary):\n",
    "    \"\"\" Standard accuracy comparing y_true vs binary predictions. \"\"\"\n",
    "    return accuracy_score(y_true, y_pred_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a5e4e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 3. XGB (Optimizing for Brier Score)\n",
    "###############################################################################\n",
    "\n",
    "def brier_scorer(estimator, X, y):\n",
    "    \"\"\"\n",
    "    Custom scikit-learn scorer for Brier:\n",
    "    we return -brier_score so GridSearchCV will 'maximize' it.\n",
    "    \"\"\"\n",
    "    prob = estimator.predict_proba(X)[:, 1]\n",
    "    return -brier_score_loss(y, prob)\n",
    "\n",
    "def train_xgb_model(X, y):\n",
    "    \"\"\"\n",
    "    Uses GridSearchCV to pick the best LightGBM hyperparams by Brier score.\n",
    "    Example param grid with monotonic constraints. \n",
    "    \"\"\"\n",
    "    # We'll do an 80/20 split for training vs. validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Suppose we assume features that end with \"_diff\" are monotonic +1\n",
    "    # If that doesn't match your data, adjust accordingly or set them all 0.\n",
    "    monotonic_constraints = []\n",
    "    for col in X_train.columns:\n",
    "        if (col == 'window_TO_avg_diff') or (col == 'window_PF_avg_diff'):\n",
    "            monotonic_constraints.append(-1)\n",
    "        elif col == 'window_clutch_count_diff':\n",
    "            monotonic_constraints.append(0)\n",
    "        elif col.endswith(\"_diff\"):\n",
    "            monotonic_constraints.append(1)\n",
    "        else:\n",
    "            monotonic_constraints.append(0)\n",
    "\n",
    "    mono_str = \"(\" + \",\".join(str(x) for x in monotonic_constraints) + \")\"\n",
    "\n",
    "    #param_grid = {\n",
    "    #    'n_estimators': [200, 500],\n",
    "    #    'learning_rate': [0.01, 0.05],\n",
    "    #    'num_leaves': [50, 100],\n",
    "    #    'max_depth': [10,50],\n",
    "    #    'min_child_samples': [10, 20],\n",
    "    #    #'monotone_constraints': [monotonic_constraints]\n",
    "    #}\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300, 500],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 5, 10],\n",
    "        'min_child_weight': [1,5, 6],\n",
    "        #'monotone_constraints': [mono_str]\n",
    "    }\n",
    "\n",
    "    scorer = make_scorer(brier_scorer, greater_is_better=True)\n",
    "    model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', random_state=42, tree_method = 'hist')\n",
    "    #lgb_model = lgb.LGBMClassifier(objective='binary', random_state=42)\n",
    "    grid = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_log_loss',\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        error_score='raise'\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train,\n",
    "             #val_set=[(X_val, y_val)],\n",
    "             #early_stopping_rounds=20,\n",
    "             verbose=True\n",
    "            )\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    # Evaluate on validation\n",
    "    val_probs = best_model.predict_proba(X_val)[:,1]\n",
    "    val_preds = (val_probs > 0.5).astype(int)\n",
    "    val_brier = brier_score(y_val, val_probs)\n",
    "    val_logloss = log_loss_metric(y_val, val_probs)\n",
    "    val_acc = accuracy_metric(y_val, val_preds)\n",
    "\n",
    "    print(\"Best XGB hyperparams:\", grid.best_params_)\n",
    "    print(f\"XGB val Brier: {val_brier:.4f}, LogLoss: {val_logloss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8866a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 4. Logistic Regression (scikit-learn) - Brier Score\n",
    "###############################################################################\n",
    "\n",
    "def train_logistic_regression(X, y):\n",
    "    \"\"\"\n",
    "    scikit-learn LogisticRegression, small param grid, picking best by Brier score.\n",
    "    \"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('scaler' , StandardScaler() ),\n",
    "        ('logreg' , LogisticRegression(random_state=42))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        'logreg__C': [0.01, 0.1, 1, 10],\n",
    "        'logreg__solver': ['lbfgs', 'liblinear'],\n",
    "        'logreg__max_iter': [100, 300, 500, 1000]\n",
    "    }\n",
    "    #scorer = make_scorer(brier_scorer, greater_is_better=True)\n",
    "    #model = LogisticRegression(random_state=42)\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_log_loss',\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "    val_probs = best_model.predict_proba(X_val)[:, 1]\n",
    "    val_preds = (val_probs > 0.5).astype(int)\n",
    "    val_brier = brier_score(y_val, val_probs)\n",
    "    val_ll = log_loss_metric(y_val, val_probs)\n",
    "    val_acc = accuracy_metric(y_val, val_preds)\n",
    "\n",
    "    print(\"Best LogisticRegression hyperparams:\", grid.best_params_)\n",
    "    print(f\"LogReg val Brier: {val_brier:.4f}, LogLoss: {val_ll:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "900091b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 5. TensorFlow Lattice (Older PWLCalibration API, custom monotonicities)\n",
    "###############################################################################\n",
    "\n",
    "# We'll define a custom Brier metric in TF\n",
    "def brier_score_tf(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "def build_tf_lattice_custom_monotonic_model_legacy_lattice_sizes(hp, feature_names, X_train):\n",
    "    \"\"\"\n",
    "    Older TF Lattice code requiring 'lattice_sizes' in Lattice(...).\n",
    "    We'll do:\n",
    "      - For each feature, a PWLCalibration with 'input_keypoints' array (no num_keypoints param).\n",
    "      - Then a Lattice layer with lattice_sizes=[2,2,...] (or [3,3,...]) plus monotonicities.\n",
    "    \"\"\"\n",
    "\n",
    "    # Suppose we define some custom monotonic map:\n",
    "    custom_monotonic_map = {\n",
    "        'window_TO_avg_diff': 'decreasing',\n",
    "        'window_PF_avg_diff': 'decreasing',\n",
    "        'window_clutch_count_diff': 'none'\n",
    "    }\n",
    "\n",
    "    # We'll pretend we have a tuner param for how many keypoints to use in PWLCalibration\n",
    "    num_keypoints = hp.Int('num_keypoints', min_value=5, max_value=15, step=5, default=10)\n",
    "    lr = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log', default=1e-3)\n",
    "\n",
    "    inputs = {}\n",
    "    calibrators = []\n",
    "    for feat in feature_names:\n",
    "        inputs[feat] = tf.keras.Input(shape=(1,), name=feat)\n",
    "\n",
    "        # monotonic direction for PWL\n",
    "        if feat in custom_monotonic_map:\n",
    "            this_monotonic = custom_monotonic_map[feat]\n",
    "        else:\n",
    "            this_monotonic = 'increasing'\n",
    "\n",
    "        # build input_keypoints array\n",
    "        f_min = float(X_train[feat].min())\n",
    "        f_max = float(X_train[feat].max())\n",
    "        keypoints = np.linspace(f_min, f_max, num_keypoints)\n",
    "\n",
    "        # PWLCalibration older signature\n",
    "        c = tfl.layers.PWLCalibration(\n",
    "            input_keypoints=keypoints,\n",
    "            units=1,\n",
    "            output_min=0.0,\n",
    "            output_max=1.0,\n",
    "            clamp_min=False,\n",
    "            clamp_max=False,\n",
    "            monotonicity=this_monotonic\n",
    "        )(inputs[feat])\n",
    "        calibrators.append(c)\n",
    "\n",
    "    # Concatenate calibrator outputs\n",
    "    concat_calibrators = tf.keras.layers.Concatenate()(calibrators)\n",
    "\n",
    "    # Next, older Lattice requires 'lattice_sizes'\n",
    "    # e.g. if you have len(feature_names)=10, you might do [2]*10 => each dimension has 2 vertices\n",
    "    n_dims = len(feature_names)\n",
    "    \n",
    "    # Build the integer monotonicities for Lattice: +1 => increasing, -1 => decreasing, 0 => none\n",
    "    lattice_monotonicities = []\n",
    "    for feat in feature_names:\n",
    "        if feat in custom_monotonic_map:\n",
    "            if custom_monotonic_map[feat] == 'increasing':\n",
    "                lattice_monotonicities.append(1)\n",
    "            elif custom_monotonic_map[feat] == 'decreasing':\n",
    "                lattice_monotonicities.append(0)\n",
    "            else:\n",
    "                lattice_monotonicities.append(0)\n",
    "        else:\n",
    "            lattice_monotonicities.append(1)\n",
    "\n",
    "    # Suppose we want 2 vertices per dimension (2^n total corners).\n",
    "    # If you have fewer features or need more resolution, you can try [3]*n_dims.\n",
    "    lattice_out = tfl.layers.Lattice(\n",
    "        lattice_sizes=[2]*n_dims,\n",
    "        monotonicities=lattice_monotonicities,\n",
    "        units=1  # for binary classification\n",
    "    )(concat_calibrators)\n",
    "\n",
    "    # Final output => Sigmoid for probability\n",
    "    outputs = tf.keras.layers.Activation('sigmoid')(lattice_out)\n",
    "    model = tf.keras.Model(inputs=list(inputs.values()), outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_tf_lattice_model_legacy_sizes(X, y):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    feature_names = X.columns.tolist()\n",
    "\n",
    "    # Keras Tuner \n",
    "    import keras_tuner as kt\n",
    "    \n",
    "    # We'll define a custom Brier metric in TF:\n",
    "    def brier_score_tf(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "    train_dict = {col: X_train[col].values for col in feature_names}\n",
    "    val_dict = {col: X_val[col].values for col in feature_names}\n",
    "\n",
    "    def model_builder(hp):\n",
    "        model = build_tf_lattice_custom_monotonic_model_legacy_lattice_sizes(\n",
    "            hp, feature_names, X_train\n",
    "        )\n",
    "        # recompile with brier metric\n",
    "        model.compile(\n",
    "            optimizer=model.optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=[\n",
    "                'accuracy',\n",
    "                tf.keras.metrics.BinaryCrossentropy(name='log_loss'),\n",
    "                brier_score_tf\n",
    "            ]\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    tuner = kt.RandomSearch(\n",
    "        model_builder,\n",
    "        objective=kt.Objective('val_brier_score_tf', direction='min'),\n",
    "        max_trials=5,\n",
    "        executions_per_trial=1,\n",
    "        project_name='tf_lattice_sizes',\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_brier_score_tf', patience=5, mode='min')\n",
    "\n",
    "    tuner.search(\n",
    "        train_dict, y_train,\n",
    "        validation_data=(val_dict, y_val),\n",
    "        epochs=50,\n",
    "        batch_size=128,\n",
    "        callbacks=[stop_early],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    best_model = tuner.hypermodel.build(best_hps)\n",
    "    best_model.fit(\n",
    "        train_dict, y_train,\n",
    "        validation_data=(val_dict, y_val),\n",
    "        epochs=50,\n",
    "        batch_size=128,\n",
    "        callbacks=[stop_early],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Evaluate final\n",
    "    res = best_model.evaluate(val_dict, y_val, verbose=0)\n",
    "    names = best_model.metrics_names\n",
    "    res_dict = dict(zip(names, res))\n",
    "    print(\"Best TF Lattice hyperparams:\", best_hps.values)\n",
    "    print(f\"Brier: {res_dict['brier_score_tf']:.4f}, log_loss: {res_dict['log_loss']:.4f}, acc: {res_dict['accuracy']:.4f}\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a233354",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 6. Ensemble\n",
    "###############################################################################\n",
    "\n",
    "def ensemble_predict_proba(X, model_xgb, model_tf, model_lr):\n",
    "    \"\"\"\n",
    "    Generates ensemble probabilities by averaging predictions from\n",
    "    XGB, TF Lattice, and LogisticRegression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix.\n",
    "    model_xgb : XGBClassifier\n",
    "        Trained XGB model.\n",
    "    model_tf : tf.keras.Model\n",
    "        Trained TensorFlow Lattice model.\n",
    "    model_lr : sklearn.linear_model.LogisticRegression or Pipeline\n",
    "        Trained Logistic Regression (or Pipeline).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Ensemble probabilities (average of the three model probabilities).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. XGB predictions\n",
    "    preds_xgb = model_xgb.predict_proba(X)[:, 1]\n",
    "\n",
    "    # 2. TF Lattice predictions\n",
    "    #   For TF Lattice, we typically pass a dictionary {feature_name: np.array(values)},\n",
    "    #   one entry per column.\n",
    "    tf_inputs = {col: X[col].values for col in X.columns}\n",
    "    preds_tf = model_tf.predict(tf_inputs).flatten()\n",
    "\n",
    "    # 3. Logistic Regression predictions\n",
    "    preds_lr = model_lr.predict_proba(X)[:, 1]\n",
    "\n",
    "    # Simple average\n",
    "    ensemble_probs = (preds_xgb + preds_tf + preds_lr) / 3.0\n",
    "    return ensemble_probs\n",
    "\n",
    "def predict_submission_ensemble(\n",
    "    dataset_path,\n",
    "    model_xgb,\n",
    "    model_tf,\n",
    "    model_lr,\n",
    "    out_filename='submission_ensemble.csv'\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a submission file for the ensemble by averaging predicted probabilities\n",
    "    from the three models: XGB, TF Lattice, and LogisticRegression.\n",
    "\n",
    "    The CSV at dataset_path must have the columns:\n",
    "    [Season, LowerTeamID, HigherTeamID, (optional Target), plus feature columns].\n",
    "\n",
    "    Output: a CSV with columns [ID, Pred], where ID = \"YYYY_LLLL_HHHH\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_path : str\n",
    "        File path to your test dataset (CSV).\n",
    "    model_xgb : XGBClassifier\n",
    "        Trained XGB model.\n",
    "    model_tf : tf.keras.Model\n",
    "        Trained TF Lattice model.\n",
    "    model_lr : sklearn.linear_model.LogisticRegression or Pipeline\n",
    "        Trained LogisticRegression (or pipeline with StandardScaler).\n",
    "    out_filename : str, optional\n",
    "        Filename for the output CSV, by default 'submission_ensemble.csv'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing [ID, Pred] for the ensemble submission.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    # Create the ID\n",
    "    df['ID'] = df.apply(\n",
    "        lambda row: f\"{int(row['Season']):04d}_{int(row['LowerTeamID']):04d}_{int(row['HigherTeamID']):04d}\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Build test feature matrix\n",
    "    feature_cols = [\n",
    "        c for c in df.columns\n",
    "        if c not in ['Season', 'LowerTeamID', 'HigherTeamID', 'Target', 'GameID', 'ID']\n",
    "    ]\n",
    "    X_test = df[feature_cols].copy()\n",
    "\n",
    "    # Ensemble predictions\n",
    "    preds = ensemble_predict_proba(X_test, model_xgb, model_tf, model_lr)\n",
    "\n",
    "    # Build submission dataframe\n",
    "    submission = pd.DataFrame({'ID': df['ID'], 'Pred': preds})\n",
    "    submission.to_csv(out_filename, index=False)\n",
    "    print(f\"Ensemble submission saved to {out_filename}\")\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e945e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 7. Predict Submission for Single Model\n",
    "###############################################################################\n",
    "\n",
    "def predict_submission(\n",
    "    model,\n",
    "    dataset_path,\n",
    "    model_type='xgb',\n",
    "    out_filename='submission.csv'\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a submission CSV for a single model of type 'xgb', 'tf', or 'lr'.\n",
    "\n",
    "    The CSV at dataset_path must have the columns:\n",
    "    [Season, LowerTeamID, HigherTeamID, (optional Target), plus feature columns].\n",
    "\n",
    "    Output: CSV with columns [ID, Pred].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : \n",
    "        - XGBClassifier if model_type='xgb'\n",
    "        - tf.keras.Model if model_type='tf'\n",
    "        - LogisticRegression (or Pipeline) if model_type='lr'\n",
    "    dataset_path : str\n",
    "        File path to your test dataset (CSV).\n",
    "    model_type : str, optional\n",
    "        One of ['xgb', 'tf', 'lr'].\n",
    "    out_filename : str, optional\n",
    "        Filename for the output CSV, by default 'submission.csv'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing [ID, Pred] for the single-model submission.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    # Create the ID for submission\n",
    "    df['ID'] = df.apply(\n",
    "        lambda row: f\"{int(row['Season']):04d}_{int(row['LowerTeamID']):04d}_{int(row['HigherTeamID']):04d}\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Build test feature matrix\n",
    "    feature_cols = [\n",
    "        c for c in df.columns\n",
    "        if c not in ['Season', 'LowerTeamID', 'HigherTeamID', 'Target', 'GameID', 'ID']\n",
    "    ]\n",
    "    X_test = df[feature_cols].copy()\n",
    "\n",
    "    # Predict probability from the correct model\n",
    "    if model_type == 'xgb':\n",
    "        probs = model.predict_proba(X_test)[:, 1]\n",
    "    elif model_type == 'tf':\n",
    "        tf_inputs = {col: X_test[col].values for col in X_test.columns}\n",
    "        probs = model.predict(tf_inputs).flatten()\n",
    "    elif model_type == 'lr':\n",
    "        probs = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        raise ValueError(\"model_type must be 'xgb', 'tf', or 'lr'.\")\n",
    "\n",
    "    # Build submission\n",
    "    submission = pd.DataFrame({'ID': df['ID'], 'Pred': probs})\n",
    "    submission.to_csv(out_filename, index=False)\n",
    "    print(f\"{model_type.upper()} submission saved to {out_filename}\")\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75c06571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dataset path if needed:\n",
    "dataset_path = \"7_game_window_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c272624",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, df = load_model_data(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1f51c2e-9d1a-4262-aaba-ab57c65fd5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target\n",
       "0    0.512318\n",
       "1    0.487682\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63e136ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Training XGB (Optimizing Brier) ====\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "Best XGB hyperparams: {'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 200}\n",
      "XGB val Brier: 0.2151, LogLoss: 0.6183, Accuracy: 0.6530\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# Train XGB\n",
    "############################################################################\n",
    "print(\"\\n==== Training XGB (Optimizing Brier) ====\")\n",
    "lgb_model = train_xgb_model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6958210d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training TF Lattice (Older PWLCalibration) ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'pwl_calibration' (type PWLCalibration).\n\nA KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n\n\nCall arguments received by layer 'pwl_calibration' (type PWLCalibration):\n  • inputs=<KerasTensor shape=(None, 1), dtype=float32, sparse=False, ragged=False, name=window_score_avg_diff>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Train TF Lattice\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Training TF Lattice (Older PWLCalibration) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m tf_lattice_model \u001b[38;5;241m=\u001b[39m train_tf_lattice_model_legacy_sizes(X, y)\n",
      "Cell \u001b[0;32mIn[6], line 128\u001b[0m, in \u001b[0;36mtrain_tf_lattice_model_legacy_sizes\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    117\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    118\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39moptimizer,\n\u001b[1;32m    119\u001b[0m         loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m         ]\n\u001b[1;32m    125\u001b[0m     )\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m--> 128\u001b[0m tuner \u001b[38;5;241m=\u001b[39m kt\u001b[38;5;241m.\u001b[39mRandomSearch(\n\u001b[1;32m    129\u001b[0m     model_builder,\n\u001b[1;32m    130\u001b[0m     objective\u001b[38;5;241m=\u001b[39mkt\u001b[38;5;241m.\u001b[39mObjective(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_brier_score_tf\u001b[39m\u001b[38;5;124m'\u001b[39m, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    131\u001b[0m     max_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m    132\u001b[0m     executions_per_trial\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    133\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf_lattice_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    134\u001b[0m     overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    137\u001b[0m stop_early \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_brier_score_tf\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    139\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch(\n\u001b[1;32m    140\u001b[0m     train_dict, y_train,\n\u001b[1;32m    141\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(val_dict, y_val),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    146\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras_tuner/src/tuners/randomsearch.py:174\u001b[0m, in \u001b[0;36mRandomSearch.__init__\u001b[0;34m(self, hypermodel, objective, max_trials, seed, hyperparameters, tune_new_entries, allow_new_entries, max_retries_per_trial, max_consecutive_failed_trials, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;241m=\u001b[39m seed\n\u001b[1;32m    164\u001b[0m oracle \u001b[38;5;241m=\u001b[39m RandomSearchOracle(\n\u001b[1;32m    165\u001b[0m     objective\u001b[38;5;241m=\u001b[39mobjective,\n\u001b[1;32m    166\u001b[0m     max_trials\u001b[38;5;241m=\u001b[39mmax_trials,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m     max_consecutive_failed_trials\u001b[38;5;241m=\u001b[39mmax_consecutive_failed_trials,\n\u001b[1;32m    173\u001b[0m )\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(oracle, hypermodel, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py:122\u001b[0m, in \u001b[0;36mTuner.__init__\u001b[0;34m(self, oracle, hypermodel, max_model_size, optimizer, loss, metrics, distribution_strategy, directory, project_name, logger, tuner_id, overwrite, executions_per_trial, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hypermodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39mrun_trial \u001b[38;5;129;01mis\u001b[39;00m Tuner\u001b[38;5;241m.\u001b[39mrun_trial:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived `hypermodel=None`. We only allow not specifying \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`hypermodel` if the user defines the search space in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Tuner.run_trial()` by subclassing a `Tuner` class without \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing a `HyperModel` instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m     )\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    123\u001b[0m     oracle\u001b[38;5;241m=\u001b[39moracle,\n\u001b[1;32m    124\u001b[0m     hypermodel\u001b[38;5;241m=\u001b[39mhypermodel,\n\u001b[1;32m    125\u001b[0m     directory\u001b[38;5;241m=\u001b[39mdirectory,\n\u001b[1;32m    126\u001b[0m     project_name\u001b[38;5;241m=\u001b[39mproject_name,\n\u001b[1;32m    127\u001b[0m     logger\u001b[38;5;241m=\u001b[39mlogger,\n\u001b[1;32m    128\u001b[0m     overwrite\u001b[38;5;241m=\u001b[39moverwrite,\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    130\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_model_size \u001b[38;5;241m=\u001b[39m max_model_size\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m optimizer\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:132\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, oracle, hypermodel, directory, project_name, overwrite, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreload()\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# Only populate initial space if not reloading.\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_populate_initial_space()\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Run in distributed mode.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dist_utils\u001b[38;5;241m.\u001b[39mhas_chief_oracle() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dist_utils\u001b[38;5;241m.\u001b[39mis_chief_oracle():\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# Proxies requests to the chief oracle.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# Avoid import at the top, to avoid inconsistent protobuf versions.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:192\u001b[0m, in \u001b[0;36mBaseTuner._populate_initial_space\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mdeclare_hyperparameters(hp)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mupdate_space(hp)\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activate_all_conditions()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:149\u001b[0m, in \u001b[0;36mBaseTuner._activate_all_conditions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m hp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_space()\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mbuild(hp)\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mupdate_space(hp)\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Update the recorded scopes.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 113\u001b[0m, in \u001b[0;36mtrain_tf_lattice_model_legacy_sizes.<locals>.model_builder\u001b[0;34m(hp)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_builder\u001b[39m(hp):\n\u001b[0;32m--> 113\u001b[0m     model \u001b[38;5;241m=\u001b[39m build_tf_lattice_custom_monotonic_model_legacy_lattice_sizes(\n\u001b[1;32m    114\u001b[0m         hp, feature_names, X_train\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# recompile with brier metric\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    118\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39moptimizer,\n\u001b[1;32m    119\u001b[0m         loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m         ]\n\u001b[1;32m    125\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[6], line 46\u001b[0m, in \u001b[0;36mbuild_tf_lattice_custom_monotonic_model_legacy_lattice_sizes\u001b[0;34m(hp, feature_names, X_train)\u001b[0m\n\u001b[1;32m     43\u001b[0m     keypoints \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(f_min, f_max, num_keypoints)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# PWLCalibration older signature\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     c \u001b[38;5;241m=\u001b[39m tfl\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mPWLCalibration(\n\u001b[1;32m     47\u001b[0m         input_keypoints\u001b[38;5;241m=\u001b[39mkeypoints,\n\u001b[1;32m     48\u001b[0m         units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     49\u001b[0m         output_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     50\u001b[0m         output_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     51\u001b[0m         clamp_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m         clamp_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     53\u001b[0m         monotonicity\u001b[38;5;241m=\u001b[39mthis_monotonic\n\u001b[1;32m     54\u001b[0m     )(inputs[feat])\n\u001b[1;32m     55\u001b[0m     calibrators\u001b[38;5;241m.\u001b[39mappend(c)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Concatenate calibrator outputs\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow_lattice/python/pwl_calibration_layer.py:466\u001b[0m, in \u001b[0;36mPWLCalibration.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    458\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lengths \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmultiply(\n\u001b[1;32m    459\u001b[0m       tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation_logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    460\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keypoint_range,\n\u001b[1;32m    461\u001b[0m       name\u001b[38;5;241m=\u001b[39mLENGTHS_NAME)\n\u001b[1;32m    462\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpolation_keypoints \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    463\u001b[0m       tf\u001b[38;5;241m.\u001b[39mcumsum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lengths, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, exclusive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    464\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keypoint_min,\n\u001b[1;32m    465\u001b[0m       name\u001b[38;5;241m=\u001b[39mINTERPOLATION_KEYPOINTS_NAME)\n\u001b[0;32m--> 466\u001b[0m interpolation_weights \u001b[38;5;241m=\u001b[39m pwl_calibration_lib\u001b[38;5;241m.\u001b[39mcompute_interpolation_weights(\n\u001b[1;32m    467\u001b[0m     inputs_to_calibration, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpolation_keypoints, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lengths)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cyclic:\n\u001b[1;32m    469\u001b[0m   \u001b[38;5;66;03m# Need to add such last height to make all heights to sum up to 0.0 in\u001b[39;00m\n\u001b[1;32m    470\u001b[0m   \u001b[38;5;66;03m# order to make calibrator cyclic.\u001b[39;00m\n\u001b[1;32m    471\u001b[0m   bias_and_heights \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[1;32m    472\u001b[0m       [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel, \u001b[38;5;241m-\u001b[39mtf\u001b[38;5;241m.\u001b[39mreduce_sum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel[\u001b[38;5;241m1\u001b[39m:], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)],\n\u001b[1;32m    473\u001b[0m       axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow_lattice/python/pwl_calibration_lib.py:114\u001b[0m, in \u001b[0;36mcompute_interpolation_weights\u001b[0;34m(inputs, keypoints, lengths)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Computes weights for PWL calibration.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m  `(batch_size, units, num_keypoints)`.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m weights \u001b[38;5;241m=\u001b[39m (inputs \u001b[38;5;241m-\u001b[39m keypoints) \u001b[38;5;241m/\u001b[39m lengths\n\u001b[0;32m--> 114\u001b[0m weights \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mminimum(weights, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    115\u001b[0m weights \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmaximum(weights, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Prepend 1.0 at the beginning to add bias unconditionally. Worth testing\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# different strategies, including those commented out, on different hardware.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/backend/common/keras_tensor.py:156\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.ops`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'pwl_calibration' (type PWLCalibration).\n\nA KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n\n\nCall arguments received by layer 'pwl_calibration' (type PWLCalibration):\n  • inputs=<KerasTensor shape=(None, 1), dtype=float32, sparse=False, ragged=False, name=window_score_avg_diff>"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# Train TF Lattice\n",
    "############################################################################\n",
    "print(\"\\n=== Training TF Lattice (Older PWLCalibration) ===\")\n",
    "tf_lattice_model = train_tf_lattice_model_legacy_sizes(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c970eb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Training Logistic Regression (Optimizing Brier) ====\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "Best LogisticRegression hyperparams: {'logreg__C': 1, 'logreg__max_iter': 100, 'logreg__solver': 'lbfgs'}\n",
      "LogReg val Brier: 0.2138, LogLoss: 0.6153, Accuracy: 0.6573\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# Train Logistic Regression\n",
    "############################################################################\n",
    "print(\"\\n==== Training Logistic Regression (Optimizing Brier) ====\")\n",
    "pt_model = train_logistic_regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a521d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Generate Submission\n",
    "############################################################################\n",
    "# Typically you'd have a separate \"test\" or future dataset for 2025 predictions\n",
    "# but here we'll just reuse the same dataset for demonstration.\n",
    "print(\"\\n==== Generating Submissions ====\")\n",
    "predict_submission(lgb_model, dataset_path, model_type='lgb', out_filename='submission_lgb.csv')\n",
    "predict_submission(tf_lattice_model, dataset_path, model_type='tf', out_filename='submission_tf.csv')\n",
    "predict_submission(pt_model, dataset_path, model_type='pt', pt_device=pt_device, out_filename='submission_pt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783467a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057fc834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b31df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e5620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
